{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lexja\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19710 7790\n"
     ]
    }
   ],
   "source": [
    "PATH = \"./semeval-2017-tweets_Subtask-A/downloaded/\"\n",
    "FILES = [PATH+f for f in listdir(PATH) if isfile(join(PATH, f))]\n",
    "DFS_train = pd.concat([pd.read_csv(file,sep=\"\\t\",names=['ID',\"label\",'text'],encoding=\"UTF-8\") for file in FILES if 'test' not in file])\n",
    "DFS_test = pd.concat([pd.read_csv(file,sep=\"\\t\",names=['ID',\"label\",'text'],encoding=\"UTF-8\") for file in FILES if 'test' in file and '2016' not in file])\n",
    "\n",
    "print(len(DFS_train),len(DFS_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to clean the data\n",
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def decontraction(text):\n",
    "    text = re.sub(r\"won\\'t\", \" will not\", text)\n",
    "    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n",
    "    text = re.sub(r\"can\\'t\", \" can not\", text)\n",
    "    text = re.sub(r\"don\\'t\", \" do not\", text)\n",
    "    \n",
    "    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n",
    "    text = re.sub(r\"ma\\'am\", \" madam\", text)\n",
    "    text = re.sub(r\"let\\'s\", \" let us\", text)\n",
    "    text = re.sub(r\"ain\\'t\", \" am not\", text)\n",
    "    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n",
    "    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n",
    "    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n",
    "    text = re.sub(r\"y\\'all\", \" you all\", text)\n",
    "\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"n\\'t've\", \" not have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'d've\", \" would have\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ll've\", \" will have\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    return text \n",
    "\n",
    "def seperate_alphanumeric(text):\n",
    "    words = text\n",
    "    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def cont_rep_char(text):\n",
    "    tchr = text.group(0) \n",
    "    \n",
    "    if len(tchr) > 1:\n",
    "        return tchr[0:2] \n",
    "\n",
    "def unique_char(rep, text):\n",
    "    substitute = re.sub(r'(\\w)\\1+', rep, text)\n",
    "    return substitute\n",
    "\n",
    "def label_to_float(label):\n",
    "    return {'positive':1.0,'neutral':0.0,'negative':-1.0}[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFS_train['text'] = DFS_train['text'].apply(lambda x : remove_url(x))\n",
    "DFS_train['text'] = DFS_train['text'].apply(lambda x : remove_punct(x))\n",
    "DFS_train['text'] = DFS_train['text'].apply(lambda x : remove_emoji(x))\n",
    "DFS_train['text'] = DFS_train['text'].apply(lambda x : decontraction(x))\n",
    "DFS_train['text'] = DFS_train['text'].apply(lambda x : seperate_alphanumeric(x))\n",
    "DFS_train['text'] = DFS_train['text'].apply(lambda x : unique_char(cont_rep_char,x))\n",
    "DFS_train['label'] = DFS_train['label'].apply(lambda x : label_to_float(x))\n",
    "\n",
    "DFS_test['text'] = DFS_test['text'].apply(lambda x : remove_url(x))\n",
    "DFS_test['text'] = DFS_test['text'].apply(lambda x : remove_punct(x))\n",
    "DFS_test['text'] = DFS_test['text'].apply(lambda x : remove_emoji(x))\n",
    "DFS_test['text'] = DFS_test['text'].apply(lambda x : decontraction(x))\n",
    "DFS_test['text'] = DFS_test['text'].apply(lambda x : seperate_alphanumeric(x))\n",
    "DFS_test['text'] = DFS_test['text'].apply(lambda x : unique_char(cont_rep_char,x))\n",
    "DFS_test['label'] = DFS_test['label'].apply(lambda x : label_to_float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "batch_size = 16\n",
    "num_samples = len(DFS_train)\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_tokens = tokenizer(\n",
    "    DFS_train['text'].tolist(), \n",
    "    max_length=seq_len, \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    add_special_tokens=True, \n",
    "    return_tensors='np'\n",
    ")\n",
    "\n",
    "labels = DFS_train['label'].values\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        train_tokens['input_ids'], \n",
    "        train_tokens['attention_mask'], \n",
    "        labels\n",
    "    )\n",
    ")\n",
    "\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': masks\n",
    "    }, labels\n",
    "\n",
    "dataset = dataset.map(map_func)\n",
    "dataset = dataset.shuffle(10000).batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "split = 0.7\n",
    "size = int((train_tokens['input_ids'].shape[0] // batch_size) * split)\n",
    "\n",
    "train_ds = dataset.take(size)\n",
    "val_ds = dataset.skip(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model_1 (TFDist  TFBaseModelOutput(l  66362880   ['input_ids[0][0]',              \n",
      " ilBertModel)                   ast_hidden_state=(N               'attention_mask[0][0]']         \n",
      "                                one, 256, 768),                                                   \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_distil_bert_model_1[0][0]'] \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 1)            513         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,757,121\n",
      "Trainable params: 394,241\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x249c1655510>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Two inputs\n",
    "input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# Transformer\n",
    "# embeddings = model.bert(input_ids, attention_mask=mask)[1]\n",
    "embeddings = model(input_ids, attention_mask=mask)[0]\n",
    "embeddings = embeddings[:, 0, :]\n",
    "# Classifier head\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(embeddings)\n",
    "# x = tf.keras.layers.Dropout(0.1)(x)\n",
    "y = tf.keras.layers.Dense(1, activation='tanh', name='outputs')(x)\n",
    "\n",
    "bert_model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# freeze bert layers\n",
    "bert_model.layers[2].trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "bert_model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "\n",
    "bert_model.summary()\n",
    "\n",
    "bert_model.load_weights(\"./bertjuh/\")\n",
    "\n",
    "# history = bert_model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=10,\n",
    "#     batch_size=batch_size\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289/1289 [==============================] - 4717s 4s/step - loss: 0.8979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8978973031044006"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_test = pd.read_csv(\"./semeval-2017-tweets_Subtask-A/downloaded/twitter-2016test-A.tsv\",sep=\"\\t\",names=['ID',\"label\",'text','nan']).drop(columns=['nan'])\n",
    "\n",
    "large_test['text'] = large_test['text'].apply(lambda x : remove_url(x))\n",
    "large_test['text'] = large_test['text'].apply(lambda x : remove_punct(x))\n",
    "large_test['text'] = large_test['text'].apply(lambda x : remove_emoji(x))\n",
    "large_test['text'] = large_test['text'].apply(lambda x : decontraction(x))\n",
    "large_test['text'] = large_test['text'].apply(lambda x : seperate_alphanumeric(x))\n",
    "large_test['text'] = large_test['text'].apply(lambda x : unique_char(cont_rep_char,x))\n",
    "large_test['label'] = large_test['label'].apply(lambda x : label_to_float(x))\n",
    "\n",
    "train_tokens = tokenizer(\n",
    "    large_test['text'].tolist(), \n",
    "    max_length=seq_len, \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    add_special_tokens=True, \n",
    "    return_tensors='np'\n",
    ")\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        train_tokens['input_ids'], \n",
    "        train_tokens['attention_mask'], \n",
    "        large_test['label']\n",
    "    )\n",
    ")\n",
    "\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': masks\n",
    "    }, large_test['label']\n",
    "\n",
    "dataset = dataset.map(map_func)\n",
    "dataset = dataset.shuffle(10000).batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "bert_model.evaluate(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486/486 [==============================] - 1826s 4s/step - loss: 0.8677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8677359819412231"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens = tokenizer(\n",
    "    DFS_test['text'].tolist(), \n",
    "    max_length=seq_len, \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    add_special_tokens=True, \n",
    "    return_tensors='np'\n",
    ")\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        train_tokens['input_ids'], \n",
    "        train_tokens['attention_mask'], \n",
    "        DFS_test['label']\n",
    "    )\n",
    ")\n",
    "\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': masks\n",
    "    }, DFS_test['label']\n",
    "\n",
    "dataset = dataset.map(map_func)\n",
    "dataset = dataset.shuffle(10000).batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "bert_model.evaluate(dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
